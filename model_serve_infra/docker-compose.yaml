version: '3'

networks:
  monitoring:
    driver: bridge

services:
  torchserve_green:
    image: pytorch/torchserve:0.12.0-cpu
    volumes:
      - ./torchserve/model_store:/home/model-server/model-store
      - ./torchserve/model_store/green:/home/model-server/model-store/green
      - ./torchserve/config.properties:/home/model-server/config.properties
      - ./torchserve/metrics.yaml:/home/model-server/metrics.yaml
      - ./torchserve/model.py:/home/model-server/model.py
      - ./torchserve/model.pth:/home/model-server/model.pth
      - ./torchserve/handler.py:/home/model-server/handler.py
      - ./torchserve/start_green.sh:/home/model-server/start_green.sh

    ports:
      - "8000:8080"  # inference API
      - "8001:8081"  # management API
      - "8002:8082"  # Metrics API
    networks:
      - monitoring 
    restart: always
    command: ["/bin/bash", "/home/model-server/start_green.sh"]
    #command: ["torchserve", "--start", "--model-store", "/home/model-server/model-store", "--enable-model-api", "--ts-config", "config.properties", "--disable-token-auth"]

  torchserve_blue:
    image: pytorch/torchserve:0.12.0-cpu
    volumes:
      - ./torchserve/model_store:/home/model-server/model-store
      - ./torchserve/model_store/blue:/home/model-server/model-store/blue
      - ./torchserve/config.properties:/home/model-server/config.properties
      - ./torchserve/metrics.yaml:/home/model-server/metrics.yaml
      - ./torchserve/model.py:/home/model-server/model.py
      - ./torchserve/model.pth:/home/model-server/model.pth
      - ./torchserve/handler.py:/home/model-server/handler.py
      - ./torchserve/start_blue.sh:/home/model-server/start_blue.sh
    ports:
      - "9000:8080"  # inference API
      - "9001:8081"  # management API
      - "9002:8082"  # Metrics API
    networks:
      - monitoring
    restart: always
    command: ["/bin/bash", "/home/model-server/start_blue.sh"]
    #command: ["torchserve", "--start", "--model-store", "/home/model-server/model-store", "--enable-model-api", "--ts-config", "config.properties", "--disable-token-auth"]

  nginx:
    container_name: nginx
    build: nginx
    volumes:
      - /var/log/nginx:/var/log/nginx/  #log compress and backup
    ports:
      - "80:80"
    networks:
      - monitoring
    depends_on:
      - torchserve_green
      - torchserve_blue
    restart: always


  prometheus:
    image: prom/prometheus
    container_name: prometheus
    ports:
      - "9090:9090"
    networks:
      - monitoring      
    volumes:
      - ./config/prometheus:/etc/prometheus
    restart: always

  node_exporter:
    image: prom/node-exporter
    container_name: node_exporter
    networks:
      - monitoring
    restart: always
    ports:
      - "9100:9100"

  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin 
      - GF_SECURITY_ADMIN_PASSWORD=grafana
    depends_on:
      - prometheus      
    volumes:
      - ./config/grafana/datasource.yml:/etc/grafana/provisioning/datasources/datasource.yml
      - ./config/grafana/dashboards.yml:/etc/grafana/provisioning/dashboards/dashboards.yml
      - ./config/grafana/dashboards:/etc/grafana/provisioning/dashboards
    networks:
      - monitoring
    restart: always


  locust:
    image: locustio/locust
    volumes:
      - ./simulator/Locust/locustfile.py:/locust-tasks/locustfile.py
    command: -f /locust-tasks/locustfile.py --host=http://172.31.29.243
    networks:
      - monitoring    
    ports:
      - "8089:8089"
    depends_on:
      - nginx

